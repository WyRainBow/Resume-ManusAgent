# ä¸Šä¸‹æ–‡ä¼ é€’é—®é¢˜è°ƒç ”æŠ¥å‘Š

## é—®é¢˜æè¿°

**ç”¨æˆ·åé¦ˆ**ï¼šæç¤ºè¯ä¸­çš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ²¡æœ‰æ­£ç¡®ä¼ é€’ç»™ LLM

**ç—‡çŠ¶**ï¼š
- Agent ä¼¼ä¹çœ‹ä¸åˆ°å½“å‰çš„ä¸Šä¸‹æ–‡çŠ¶æ€ï¼ˆå¦‚"ç®€å†å·²åŠ è½½"ï¼‰
- Agent é‡å¤æ‰§è¡Œç›¸åŒçš„æ“ä½œï¼ˆå¦‚é‡å¤åŠ è½½ç®€å†ï¼‰
- æç¤ºè¯ä¸­çš„ `{context}` å˜é‡ä¼¼ä¹ä¸èµ·ä½œç”¨

---

## ä¸€ã€å®Œæ•´çš„è°ƒç”¨é“¾è·¯

```
å‰ç«¯ WebSocket è¯·æ±‚
    â†“
server.py: websocket_endpoint()
    â†“
åˆ›å»º Manus æ™ºèƒ½ä½“
    â†“
server.py: ç”¨æˆ·æ¶ˆæ¯æ·»åŠ åˆ° memory å’Œ ChatHistory
    â†“
æ‰‹åŠ¨æ‰§è¡Œæ­¥éª¤å¾ªç¯ (server.py: 419-499)
    â†“
agent.step() è°ƒç”¨é“¾ï¼š
    â†“
manus.py: think() â†’ _generate_dynamic_prompts()
    â†“
ç”Ÿæˆ {directory} å’Œ {context} å˜é‡
    â†“
manus.py: system_prompt = SYSTEM_PROMPT.format()
    â†“
toolcall.py: think() â†’ llm.ask_tool()
    â†“
llm.py: ask_tool() â†’ format_messages() â†’ API è°ƒç”¨
```

---

## äºŒã€ä¸Šä¸‹æ–‡ç”Ÿæˆæœºåˆ¶

### 2.1 ä¸Šä¸‹æ–‡ç”Ÿæˆä½ç½®

**æ–‡ä»¶**: `app/agent/manus.py`

**æ–¹æ³•**: `_generate_dynamic_prompts()` (ç¬¬ 197-284 è¡Œ)

```python
async def _generate_dynamic_prompts(self, user_input: str) -> tuple[str, str]:
    """ç”ŸæˆåŠ¨æ€æç¤ºè¯ï¼ŒåŒ…å«å½“å‰ä¸Šä¸‹æ–‡ä¿¡æ¯"""

    # 1. æ”¶é›†ä¸Šä¸‹æ–‡ä¿¡æ¯
    context_parts = []
    if self._conversation_state.context.resume_loaded:
        context_parts.append("âœ… ç®€å†å·²åŠ è½½")
    else:
        context_parts.append("âš ï¸ ç®€å†æœªåŠ è½½ï¼Œå»ºè®®å…ˆåŠ è½½ç®€å†")

    if self._current_resume_path:
        context_parts.append(f"ğŸ“„ å½“å‰ç®€å†æ–‡ä»¶: {self._current_resume_path}")

    # 2. ç»„è£…ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
    context = "\n".join(context_parts) if context_parts else "åˆå§‹çŠ¶æ€"

    # 3. å¡«å……ç³»ç»Ÿæç¤ºè¯æ¨¡æ¿
    system_prompt = SYSTEM_PROMPT.format(
        directory=config.workspace_root,
        context=context
    )

    # 4. å¡«å……ä¸‹ä¸€æ­¥æç¤ºè¯æ¨¡æ¿
    next_step_prompt = NEXT_STEP_PROMPT.format(
        current_dir=self.working_dir
    )

    return system_prompt, next_step_prompt
```

### 2.2 ç³»ç»Ÿæç¤ºè¯æ¨¡æ¿

**æ–‡ä»¶**: `app/prompt/manus.py`

```python
SYSTEM_PROMPT = """You are OpenManus, an AI assistant for resume optimization.

...

Current directory: {directory}
Current state: {context}
"""
```

---

## ä¸‰ã€æ¶ˆæ¯ä¼ é€’é“¾åˆ†æ

### 3.1 Manus.think() æµç¨‹

**æ–‡ä»¶**: `app/agent/manus.py`

```python
async def think(self) -> bool:
    # 1. ç”ŸæˆåŠ¨æ€æç¤ºè¯ï¼ˆåŒ…å«ä¸Šä¸‹æ–‡ï¼‰
    self.system_prompt, self.next_step_prompt = await self._generate_dynamic_prompts(user_input)

    # 2. è°ƒç”¨çˆ¶ç±»çš„ think() æ–¹æ³•
    return await super().think()
```

### 3.2 ToolCallAgent.think() æµç¨‹

**æ–‡ä»¶**: `app/agent/toolcall.py`

```python
async def think(self) -> bool:
    # 1. å°† next_step_prompt æ·»åŠ ä¸º user æ¶ˆæ¯
    if self.next_step_prompt:
        user_msg = Message.user_message(self.next_step_prompt)
        self.messages += [user_msg]

    # 2. è°ƒç”¨ LLMï¼Œä¼ å…¥ system_msgs
    response = await self.llm.ask_tool(
        messages=self.messages,
        system_msgs=(
            [Message.system_message(self.system_prompt)]
            if self.system_prompt
            else None
        ),
        ...
    )
```

### 3.3 LLM æ¶ˆæ¯æ ¼å¼åŒ–

**æ–‡ä»¶**: `app/llm.py`

```python
async def ask_tool(self, messages, system_msgs=None, ...):
    # æ ¼å¼åŒ–æ¶ˆæ¯
    if system_msgs:
        system_msgs = self.format_messages(system_msgs, supports_images)
        messages = system_msgs + self.format_messages(messages, supports_images)
    else:
        messages = self.format_messages(messages, supports_images)

    # å‘é€ç»™ API
    response = await self.client.chat.completions.create(
        messages=messages,
        ...
    )
```

---

## å››ã€å‘ç°çš„é—®é¢˜

### é—®é¢˜ 1: ä¸Šä¸‹æ–‡ä¿¡æ¯è¢«åˆ†å‰² âš ï¸

**ä½ç½®**: `app/agent/toolcall.py:41-43`

```python
if self.next_step_prompt:
    user_msg = Message.user_message(self.next_step_prompt)
    self.messages += [user_msg]
```

**é—®é¢˜åˆ†æ**ï¼š
- `system_prompt` åŒ…å« `{directory}` å’Œ `{context}` å˜é‡
- `next_step_prompt` è¢«æ·»åŠ ä¸º user æ¶ˆæ¯
- ä¸¤è€…åˆ†åˆ«ä¼ é€’ï¼Œå¯èƒ½å¯¼è‡´ LLM å¿½ç•¥ system ä¸­çš„ä¸Šä¸‹æ–‡

### é—®é¢˜ 2: æ¶ˆæ¯ç´¯ç§¯é—®é¢˜ âš ï¸

**ä½ç½®**: `app/agent/toolcall.py`

```python
self.messages += [user_msg]  # æ¯æ¬¡éƒ½è¿½åŠ ï¼Œä¸æ¸…ç†
```

**é—®é¢˜åˆ†æ**ï¼š
- æ¶ˆæ¯åˆ—è¡¨ä¸æ–­ç´¯ç§¯
- å¤šè½®å¯¹è¯åï¼Œä¸Šä¸‹æ–‡ä¿¡æ¯è¢«åŸ‹åœ¨å¤§é‡å†å²æ¶ˆæ¯ä¸­
- LLM å¯èƒ½éš¾ä»¥æ³¨æ„åˆ°æœ€æ–°çš„ä¸Šä¸‹æ–‡å˜åŒ–

### é—®é¢˜ 3: ç¼ºä¹è°ƒè¯•æ‰‹æ®µ âš ï¸

**é—®é¢˜åˆ†æ**ï¼š
- æ— æ³•è¿½è¸ªä¸Šä¸‹æ–‡æ˜¯å¦æ­£ç¡®ç”Ÿæˆ
- æ— æ³•ç¡®è®¤ LLM æ˜¯å¦æ”¶åˆ°äº†æ ¼å¼åŒ–åçš„ä¸Šä¸‹æ–‡
- è°ƒè¯•å›°éš¾

---

## äº”ã€ä¿®å¤æ–¹æ¡ˆ

### æ–¹æ¡ˆ 1: å¢å¼º System Prompt ä¼˜å…ˆçº§

**ç›®æ ‡**: ç¡®ä¿ä¸Šä¸‹æ–‡ä¿¡æ¯åœ¨ System Prompt ä¸­çªå‡ºæ˜¾ç¤º

**ä¿®æ”¹æ–‡ä»¶**: `app/prompt/manus.py`

```python
SYSTEM_PROMPT = """You are OpenManus, an AI assistant for resume optimization.

## Current State (IMPORTANT - Check this first):
{context}

## Working Directory:
{directory}

## Available Tools:
...

## Rules:
- ALWAYS check Current State before taking action
- If resume is already loaded (âœ…), do NOT load it again
- If resume is NOT loaded (âš ï¸), load it first
"""
```

### æ–¹æ¡ˆ 2: æ·»åŠ è°ƒè¯•æ—¥å¿—

**ä¿®æ”¹æ–‡ä»¶**: `app/agent/manus.py`

```python
async def _generate_dynamic_prompts(self, user_input: str) -> tuple[str, str]:
    # ... ç”Ÿæˆ context ...

    # æ·»åŠ è°ƒè¯•æ—¥å¿—
    logger.info(f"ğŸ“ [ä¸Šä¸‹æ–‡ç”Ÿæˆ] context: {context}")
    logger.info(f"ğŸ“ [ä¸Šä¸‹æ–‡ç”Ÿæˆ] system_prompt:\n{system_prompt[:500]}...")

    return system_prompt, next_step_prompt
```

**ä¿®æ”¹æ–‡ä»¶**: `app/llm.py`

```python
async def ask_tool(self, messages, system_msgs=None, ...):
    # ... æ ¼å¼åŒ–æ¶ˆæ¯ ...

    # æ·»åŠ è°ƒè¯•æ—¥å¿—
    logger.info(f"ğŸ“ [LLMè°ƒç”¨] å‘é€ {len(messages)} æ¡æ¶ˆæ¯")
    for i, msg in enumerate(messages):
        logger.info(f"  [{i}] {msg['role']}: {msg['content'][:100]}...")

    response = await self.client.chat.completions.create(...)
```

### æ–¹æ¡ˆ 3: åœ¨ Next Step Prompt ä¸­é‡å¤å…³é”®ä¸Šä¸‹æ–‡

**ä¿®æ”¹æ–‡ä»¶**: `app/prompt/manus.py`

```python
NEXT_STEP_PROMPT = """## Decision Checklist:

1. Check Current State:
   - If "âœ… ç®€å†å·²åŠ è½½" â†’ Proceed with analysis
   - If "âš ï¸ ç®€å†æœªåŠ è½½" â†’ Load resume first

2. Match user intent to tool:
   | User Request | Tool | Action |
   |--------------|------|--------|
   | åŠ è½½/è¯»å–ç®€å† + path | cv_reader_agent | Load, confirm, stop |
   | åˆ†ææ•™è‚²èƒŒæ™¯ | education_analyzer | Analyze education |
   | åˆ†æç®€å† | cv_analyzer_agent | Full analysis |

3. Execute the matching tool, then stop.

Current context: {context}
"""
```

---

## å…­ã€éªŒè¯æ–¹æ³•

### 6.1 æ·»åŠ æ—¥å¿—éªŒè¯

åœ¨å…³é”®ä½ç½®æ·»åŠ æ—¥å¿—åï¼Œè§‚å¯Ÿè¾“å‡ºï¼š

```
ğŸ“ [ä¸Šä¸‹æ–‡ç”Ÿæˆ] context: âœ… ç®€å†å·²åŠ è½½
ğŸ“„ å½“å‰ç®€å†æ–‡ä»¶: /path/to/resume.md

ğŸ“ [LLMè°ƒç”¨] å‘é€ 5 æ¡æ¶ˆæ¯
  [0] system: You are OpenManus...
  [1] user: åŠ è½½ç®€å† /path/to/resume.md
  [2] assistant: [è°ƒç”¨å·¥å…·]
  [3] tool: {"resume": {...}}
  [4] user: ## Decision Checklist...
```

### 6.2 åŠŸèƒ½éªŒè¯

1. **åœºæ™¯ 1**: åŠ è½½ç®€å†åå†æ¬¡è¯·æ±‚åˆ†æ
   - é¢„æœŸï¼šç›´æ¥åˆ†æï¼Œä¸é‡å¤åŠ è½½
   - æ£€æŸ¥ï¼šæ—¥å¿—ä¸­æ˜¯å¦æ˜¾ç¤º"âœ… ç®€å†å·²åŠ è½½"

2. **åœºæ™¯ 2**: æœªåŠ è½½ç®€å†æ—¶è¯·æ±‚åˆ†æ
   - é¢„æœŸï¼šå…ˆåŠ è½½ç®€å†
   - æ£€æŸ¥ï¼šæ—¥å¿—ä¸­æ˜¯å¦æ˜¾ç¤º"âš ï¸ ç®€å†æœªåŠ è½½"

---

## ä¸ƒã€æ€»ç»“

| é—®é¢˜ | å½±å“ | ä¼˜å…ˆçº§ |
|------|------|--------|
| ä¸Šä¸‹æ–‡ä¿¡æ¯è¢«åˆ†å‰² | LLM å¯èƒ½å¿½ç•¥ä¸Šä¸‹æ–‡ | é«˜ |
| æ¶ˆæ¯ç´¯ç§¯ | å†å²æ¶ˆæ¯è¿‡å¤šï¼Œä¸Šä¸‹æ–‡è¢«åŸ‹ | ä¸­ |
| ç¼ºä¹è°ƒè¯•æ‰‹æ®µ | æ— æ³•è¿½è¸ªé—®é¢˜ | é«˜ |

**æ¨èä¿®å¤é¡ºåº**ï¼š
1. æ·»åŠ è°ƒè¯•æ—¥å¿—ï¼ˆå¿«é€Ÿå®šä½é—®é¢˜ï¼‰
2. å¢å¼º System Prompt ä¸­çš„ä¸Šä¸‹æ–‡æ˜¾ç¤º
3. åœ¨ Next Step Prompt ä¸­é‡å¤å…³é”®çŠ¶æ€ä¿¡æ¯

---

## å…«ã€ç›¸å…³æ–‡ä»¶

| æ–‡ä»¶ | ä½œç”¨ |
|------|------|
| `app/agent/manus.py` | ä¸Šä¸‹æ–‡ç”Ÿæˆã€Agent ä¸»é€»è¾‘ |
| `app/agent/toolcall.py` | æ¶ˆæ¯ä¼ é€’ã€LLM è°ƒç”¨ |
| `app/prompt/manus.py` | ç³»ç»Ÿæç¤ºè¯æ¨¡æ¿ |
| `app/llm.py` | LLM API è°ƒç”¨ã€æ¶ˆæ¯æ ¼å¼åŒ– |
| `app/web/server.py` | WebSocket ç«¯ç‚¹ã€å‰ç«¯äº¤äº’ |
