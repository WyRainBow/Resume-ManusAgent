"""
Conversation State Manager - Manages conversation state and intent recognition

This module preserves the logic from the original conversation_manager.py,
separated from the message history management.
"""

from enum import Enum
from typing import Any, Dict, List, Optional, Tuple
from pydantic import BaseModel, Field
from datetime import datetime
import json

from app.logger import logger


class ConversationState(str, Enum):
    """å¯¹è¯çŠ¶æ€"""
    IDLE = "idle"
    GREETING = "greeting"
    RESUME_LOADED = "resume_loaded"
    ANALYZING = "analyzing"
    OPTIMIZING = "optimizing"
    WAITING_ANSWER = "waiting_answer"
    EDITING = "editing"


class Intent(str, Enum):
    """ç”¨æˆ·æ„å›¾"""
    GREETING = "greeting"
    LOAD_RESUME = "load_resume"
    VIEW_RESUME = "view_resume"
    ANALYZE = "analyze"
    OPTIMIZE = "optimize"
    OPTIMIZE_SECTION = "optimize_section"
    ANSWER_QUESTION = "answer_question"
    CONFIRM = "confirm"
    CANCEL = "cancel"
    UNKNOWN = "unknown"


class OptimizationContext(BaseModel):
    """ä¼˜åŒ–ä¸Šä¸‹æ–‡ - è¿½è¸ªä¼˜åŒ–æµç¨‹çŠ¶æ€"""
    section: str = ""
    current_question: int = 0
    answers: Dict[str, str] = Field(default_factory=dict)
    started_at: Optional[datetime] = None


class ConversationContext(BaseModel):
    """å¯¹è¯ä¸Šä¸‹æ–‡"""
    state: ConversationState = ConversationState.IDLE
    resume_loaded: bool = False
    last_tool_used: str = ""
    last_ai_response: str = ""
    optimization: OptimizationContext = Field(default_factory=OptimizationContext)
    history_summary: str = ""
    turn_count: int = 0


class ConversationStateManager:
    """
    å¯¹è¯çŠ¶æ€ç®¡ç†å™¨

    ä¸åŸ ConversationManager çš„åŒºåˆ«ï¼š
    - ä¸ç®¡ç†æ¶ˆæ¯å†å²ï¼ˆç”± ChatHistoryManager è´Ÿè´£ï¼‰
    - åªè´Ÿè´£çŠ¶æ€æœºå’Œæ„å›¾è¯†åˆ«
    """

    def __init__(self, llm=None):
        """
        åˆå§‹åŒ–å¯¹è¯çŠ¶æ€ç®¡ç†å™¨

        Args:
            llm: LLM å®¢æˆ·ç«¯å®ä¾‹ï¼Œç”¨äºæ„å›¾è¯†åˆ«
        """
        self.context = ConversationContext()
        self.llm = llm

    async def classify_intent_with_llm(
        self,
        user_input: str,
        conversation_history: List[Any] = None,
        last_ai_message: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨ LLM è¿›è¡Œæ„å›¾åˆ†ç±»

        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            conversation_history: å¯¹è¯å†å²ï¼ˆMessage å¯¹è±¡åˆ—è¡¨ï¼‰
            last_ai_message: æœ€åä¸€æ¡ AI æ¶ˆæ¯å†…å®¹

        Returns:
            {
                "intent": Intent,
                "confidence": float,
                "extracted_info": {
                    "section": str,
                    "question": str,
                    "answer_type": str
                },
                "reasoning": str
            }
        """
        if not self.llm:
            logger.warning("LLM å®¢æˆ·ç«¯æœªè®¾ç½®ï¼Œå›é€€åˆ°é»˜è®¤æ„å›¾")
            return {
                "intent": Intent.UNKNOWN,
                "confidence": 0.0,
                "extracted_info": {},
                "reasoning": "LLM å®¢æˆ·ç«¯æœªè®¾ç½®"
            }

        # æ„å»ºå¯¹è¯å†å²æ‘˜è¦
        history_text = ""
        if conversation_history:
            recent_messages = conversation_history[-5:]
            history_parts = []
            for msg in recent_messages:
                if hasattr(msg, 'role') and hasattr(msg, 'content'):
                    role = "ç”¨æˆ·" if msg.role == "user" else "AI"
                    content = msg.content[:200] if msg.content else ""
                    if content:
                        history_parts.append(f"{role}: {content}")
            history_text = "\n".join(history_parts)

        # æ„å»ºæ„å›¾è¯†åˆ«æç¤ºè¯
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ„å›¾è¯†åˆ«åŠ©æ‰‹ã€‚æ ¹æ®ç”¨æˆ·è¾“å…¥å’Œå¯¹è¯ä¸Šä¸‹æ–‡ï¼Œå‡†ç¡®è¯†åˆ«ç”¨æˆ·çš„çœŸå®æ„å›¾ã€‚

## å¯¹è¯å†å²
{history_text if history_text else "æ— "}

## æœ€åä¸€æ¡AIæ¶ˆæ¯
{last_ai_message if last_ai_message else "æ— "}

## ç”¨æˆ·å½“å‰è¾“å…¥
"{user_input}"

## æ„å›¾ç±»å‹è¯´æ˜
- greeting: é—®å€™ï¼ˆä½ å¥½ã€hiã€helloç­‰ï¼‰
- load_resume: åŠ è½½ç®€å†ï¼ˆåŠ è½½ã€ä¸Šä¼ ã€å¯¼å…¥ç®€å†ç­‰ï¼‰
- view_resume: æŸ¥çœ‹/ä»‹ç»ç®€å†ï¼ˆçœ‹çœ‹ç®€å†ã€ä»‹ç»ç®€å†ã€ç®€å†å†…å®¹ç­‰ï¼‰
- analyze: åˆ†æç®€å†ï¼ˆåˆ†æã€è¯Šæ–­ã€è¯„ä¼°ç®€å†ç­‰ï¼‰
- optimize: ä¼˜åŒ–ç®€å†ï¼ˆæ•´ä½“ä¼˜åŒ–ï¼Œä¸æŒ‡å®šå…·ä½“æ¨¡å—ï¼‰
- optimize_section: ä¼˜åŒ–ç‰¹å®šæ¨¡å—ï¼ˆå¦‚"ä¼˜åŒ–å·¥ä½œç»å†"ã€"ä¼˜åŒ–ä¸ªäººæ€»ç»“"ã€"ä¼˜åŒ–æŠ€èƒ½"ç­‰ï¼‰
- answer_question: å›ç­”AIçš„é—®é¢˜ï¼ˆå½“AIé—®äº†"é—®é¢˜1"ã€"é—®é¢˜2"ã€"é—®é¢˜3"åï¼Œç”¨æˆ·çš„å›ç­”ï¼‰
- confirm: ç¡®è®¤ï¼ˆå¯ä»¥ã€å¥½çš„ã€ç¡®è®¤ã€å¼€å§‹ã€ç»§ç»­ç­‰ç®€çŸ­ç¡®è®¤è¯ï¼‰
- cancel: å–æ¶ˆï¼ˆå–æ¶ˆã€ä¸è¦ã€ç®—äº†ã€åœæ­¢ç­‰ï¼‰
- unknown: å…¶ä»–æœªçŸ¥æ„å›¾

## è¯†åˆ«è§„åˆ™
1. **å›ç­”è¯†åˆ«**ï¼šå¦‚æœæœ€åä¸€æ¡AIæ¶ˆæ¯åŒ…å«"é—®é¢˜1"ã€"é—®é¢˜2"ã€"é—®é¢˜3"ï¼Œä¸”ç”¨æˆ·è¾“å…¥æ˜¯å›ç­”ï¼ˆä¸æ˜¯æ–°é—®é¢˜ï¼‰ï¼Œåˆ™è¯†åˆ«ä¸º answer_question
2. **æ¨¡å—ä¼˜åŒ–è¯†åˆ«**ï¼šå¦‚æœç”¨æˆ·è¯´"ä¼˜åŒ–XX"ï¼ˆXXæ˜¯å…·ä½“æ¨¡å—åï¼‰ï¼Œåˆ™è¯†åˆ«ä¸º optimize_section
3. **ç¡®è®¤è¯†åˆ«**ï¼šå¦‚æœç”¨æˆ·è¾“å…¥æ˜¯ç®€çŸ­ç¡®è®¤è¯ï¼ˆ1-3ä¸ªå­—ï¼‰ï¼Œä¸”ä¸Šä¸‹æ–‡ä¸­æœ‰å¾…ç¡®è®¤çš„å†…å®¹ï¼Œåˆ™è¯†åˆ«ä¸º confirm
4. **ä¸Šä¸‹æ–‡ç†è§£**ï¼šå¿…é¡»è€ƒè™‘å¯¹è¯å†å²ï¼Œä¸è¦åªçœ‹å½“å‰è¾“å…¥

## è¾“å‡ºæ ¼å¼ï¼ˆå¿…é¡»æ˜¯æœ‰æ•ˆçš„JSONï¼‰
{{
    "intent": "æ„å›¾ç±»å‹ï¼ˆå°å†™ï¼‰",
    "confidence": 0.0-1.0,
    "extracted_info": {{
        "section": "æ¨¡å—åï¼ˆå¦‚æœæ˜¯optimize_sectionï¼Œå¦‚ï¼šå·¥ä½œç»å†ã€ä¸ªäººæ€»ç»“ï¼‰",
        "question": "é—®é¢˜ç¼–å·ï¼ˆå¦‚æœæ˜¯answer_questionï¼Œå¦‚ï¼šé—®é¢˜1ã€é—®é¢˜2ã€é—®é¢˜3ï¼‰",
        "answer_type": "å›ç­”ç±»å‹ï¼ˆå¦‚æœæ˜¯answer_questionï¼šduties/results/technologiesï¼‰"
    }},
    "reasoning": "è¯†åˆ«ç†ç”±ï¼ˆç®€çŸ­ï¼Œ1-2å¥è¯ï¼‰"
}}

è¯·åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""

        try:
            response = await self.llm.ask(
                messages=[{"role": "user", "content": prompt}],
                stream=False,
                temperature=0.1
            )

            response = response.strip()
            if response.startswith("```json"):
                response = response[7:]
            if response.startswith("```"):
                response = response[3:]
            if response.endswith("```"):
                response = response[:-3]
            response = response.strip()

            result = json.loads(response)

            intent_str = result.get("intent", "unknown")
            try:
                intent = Intent(intent_str)
            except ValueError:
                logger.warning(f"æœªçŸ¥çš„æ„å›¾ç±»å‹: {intent_str}ï¼Œä½¿ç”¨ UNKNOWN")
                intent = Intent.UNKNOWN

            return {
                "intent": intent,
                "confidence": result.get("confidence", 0.5),
                "extracted_info": result.get("extracted_info", {}),
                "reasoning": result.get("reasoning", "")
            }

        except json.JSONDecodeError as e:
            logger.error(f"LLM è¿”å›çš„ JSON è§£æå¤±è´¥: {e}")
            return {
                "intent": Intent.UNKNOWN,
                "confidence": 0.0,
                "extracted_info": {},
                "reasoning": f"JSON è§£æå¤±è´¥: {str(e)}"
            }
        except Exception as e:
            logger.error(f"LLM æ„å›¾è¯†åˆ«å¤±è´¥: {e}")
            return {
                "intent": Intent.UNKNOWN,
                "confidence": 0.0,
                "extracted_info": {},
                "reasoning": f"è¯†åˆ«å¤±è´¥: {str(e)}"
            }

    async def detect_intent(
        self,
        user_input: str,
        conversation_history: List[Any] = None,
        last_ai_message: Optional[str] = None
    ) -> Tuple[Intent, Dict[str, Any]]:
        """ä½¿ç”¨ LLM æ£€æµ‹ç”¨æˆ·æ„å›¾"""
        llm_result = await self.classify_intent_with_llm(
            user_input=user_input,
            conversation_history=conversation_history,
            last_ai_message=last_ai_message
        )

        intent = llm_result["intent"]
        extracted_info = llm_result.get("extracted_info", {})

        logger.info(f"ğŸ§  LLM æ„å›¾è¯†åˆ«: {intent.value}, ç½®ä¿¡åº¦: {llm_result.get('confidence', 0):.2f}")

        return intent, extracted_info

    async def process_input(
        self,
        user_input: str,
        conversation_history: List[Any] = None,
        last_ai_message: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        å¤„ç†ç”¨æˆ·è¾“å…¥ï¼Œè¿”å›å¤„ç†å»ºè®®

        Returns:
            {
                "intent": Intent,
                "tool": str,
                "tool_args": dict,
                "context_prompt": str,
                "should_skip_llm": bool,
            }
        """
        self.context.turn_count += 1

        intent, info = await self.detect_intent(
            user_input=user_input,
            conversation_history=conversation_history,
            last_ai_message=last_ai_message
        )

        result = {
            "intent": intent,
            "tool": None,
            "tool_args": {},
            "context_prompt": "",
            "should_skip_llm": False,
        }

        if intent == Intent.GREETING:
            result["tool"] = None
            self.context.state = ConversationState.GREETING
        elif intent == Intent.LOAD_RESUME:
            # åŠ è½½ç®€å† â†’ è°ƒç”¨ cv_reader_agent
            result["tool"] = "cv_reader_agent"
            # å¦‚æœ extracted_info ä¸­æœ‰æ–‡ä»¶è·¯å¾„ï¼Œä½¿ç”¨å®ƒ
            if info.get("file_path"):
                result["tool_args"] = {"file_path": info["file_path"]}
        elif intent == Intent.VIEW_RESUME:
            result["tool"] = "cv_reader_agent"
        elif intent == Intent.ANALYZE:
            result["tool"] = "cv_analyzer_agent"
            self.context.state = ConversationState.ANALYZING
        elif intent == Intent.OPTIMIZE:
            # ä¼˜åŒ–è¯·æ±‚ â†’ å…ˆç”¨ Analyzer åˆ†æå¹¶ç»™å‡ºå»ºè®®
            result["tool"] = "cv_analyzer_agent"
            result["tool_args"] = {"mode": "optimize"}
            self.context.state = ConversationState.OPTIMIZING
        elif intent == Intent.OPTIMIZE_SECTION:
            # ä¼˜åŒ–ç‰¹å®šæ¨¡å— â†’ å…ˆç”¨ Analyzer åˆ†æè¯¥æ¨¡å—
            section = info.get("section", "å·¥ä½œç»å†")
            result["tool"] = "cv_analyzer_agent"
            result["tool_args"] = {
                "mode": "optimize_section",
                "section": section
            }
            self.context.state = ConversationState.OPTIMIZING
            self.context.optimization.section = section
            self.context.optimization.started_at = datetime.now()
        elif intent == Intent.CONFIRM:
            result = self._handle_confirm()
        elif intent == Intent.CANCEL:
            self._reset_optimization()
            result["context_prompt"] = "ç”¨æˆ·å–æ¶ˆäº†å½“å‰æ“ä½œã€‚"
        else:
            result["context_prompt"] = self._generate_context_prompt()

        return result

    def _handle_confirm(self) -> Dict[str, Any]:
        """å¤„ç†ç¡®è®¤æ„å›¾ - ç”¨æˆ·åŒæ„å»ºè®®åç›´æ¥ç¼–è¾‘"""
        result = {
            "intent": Intent.CONFIRM,
            "tool": None,
            "tool_args": {},
            "context_prompt": "",
            "should_skip_llm": False,
        }

        last_tool = self.context.last_tool_used
        last_response = self.context.last_ai_response

        # å¦‚æœæ˜¯ Analyzer åˆ†æåç”¨æˆ·ç¡®è®¤ï¼Œåˆ™è°ƒç”¨ Editor ç›´æ¥åº”ç”¨ä¿®æ”¹
        if "analyzer" in last_tool or "åˆ†æ" in last_response:
            result["tool"] = "cv_editor_agent"
            # æ ¹æ®ä¸Šä¸€æ¡ AI å“åº”åˆ¤æ–­è¦ç¼–è¾‘å“ªä¸ªæ¨¡å—
            if "å·¥ä½œç»å†" in last_response:
                result["tool_args"] = {"action": "edit", "section": "å·¥ä½œç»å†"}
            elif "ä¸ªäººæ€»ç»“" in last_response:
                result["tool_args"] = {"action": "edit", "section": "ä¸ªäººæ€»ç»“"}
            elif "æŠ€èƒ½" in last_response:
                result["tool_args"] = {"action": "edit", "section": "æŠ€èƒ½"}
            else:
                result["tool_args"] = {"action": "auto_apply"}

        return result

    def _generate_context_prompt(self) -> str:
        """ç”Ÿæˆä¸Šä¸‹æ–‡æç¤º"""
        parts = []

        parts.append(f"å½“å‰çŠ¶æ€: {self.context.state.value}")

        if self.context.resume_loaded:
            parts.append("ç®€å†å·²åŠ è½½")
        else:
            parts.append("ç®€å†æœªåŠ è½½")

        if self.context.state in [ConversationState.OPTIMIZING, ConversationState.WAITING_ANSWER]:
            opt = self.context.optimization
            if opt.section:
                parts.append(f"æ­£åœ¨ä¼˜åŒ–: {opt.section}")
                parts.append(f"å½“å‰é—®é¢˜: é—®é¢˜{opt.current_question}")

        return "\n".join(parts)

    def update_after_tool(self, tool_name: str, result: str):
        """å·¥å…·æ‰§è¡Œåæ›´æ–°çŠ¶æ€"""
        self.context.last_tool_used = tool_name
        self.context.last_ai_response = result[:500]

        if "æˆ‘æœ€å»ºè®®å…ˆå›ç­”é—®é¢˜" in result or "è¯·å›ç­”" in result:
            self.context.state = ConversationState.WAITING_ANSWER
            import re
            match = re.search(r'é—®é¢˜[ä¸€äºŒä¸‰123]', result)
            if match:
                q_map = {"ä¸€": 1, "äºŒ": 2, "ä¸‰": 3, "1": 1, "2": 2, "3": 3}
                q_char = match.group().replace("é—®é¢˜", "")
                self.context.optimization.current_question = q_map.get(q_char, 1)

    def update_resume_loaded(self, loaded: bool):
        """æ›´æ–°ç®€å†åŠ è½½çŠ¶æ€"""
        self.context.resume_loaded = loaded
        if loaded:
            self.context.state = ConversationState.RESUME_LOADED

    def _reset_optimization(self):
        """é‡ç½®ä¼˜åŒ–çŠ¶æ€"""
        self.context.optimization = OptimizationContext()
        self.context.state = ConversationState.RESUME_LOADED if self.context.resume_loaded else ConversationState.IDLE

    def get_state_for_prompt(self) -> str:
        """è·å–ç”¨äºæç¤ºè¯çš„çŠ¶æ€æè¿°"""
        return self._generate_context_prompt()

    def should_use_tool_directly(self, intent: Intent) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ç›´æ¥ä½¿ç”¨å·¥å…·"""
        direct_intents = [
            Intent.LOAD_RESUME,
            Intent.VIEW_RESUME,
            Intent.ANALYZE,
            Intent.OPTIMIZE,
            Intent.OPTIMIZE_SECTION,
            Intent.ANSWER_QUESTION,
        ]
        return intent in direct_intents
