"""
OpenManus Web Server - Refactored modular architecture.

This server provides:
- WebSocket endpoint for agent interaction with real-time streaming
- HTTP API for resume data management
- HTTP API for chat history and checkpoint management

Architecture:
- Uses ConnectionManager for WebSocket connection lifecycle
- Uses SessionManager for agent session management
- Uses MessageHandler for WebSocket message routing
- Uses StreamProcessor for agent execution streaming
- Uses modular routes for HTTP API endpoints
"""

import asyncio
import json
import os
import uuid
from datetime import datetime
from pathlib import Path
from typing import Any

from fastapi import FastAPI, WebSocket, WebSocketDisconnect, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel

from app.agent.manus import Manus
from app.llm import LLM
from app.logger import logger
from app.schema import AgentState, Message, Memory, Role

# Import refactored modules
from app.web.websocket.connection_manager import connection_manager
from app.web.websocket.session_manager import session_manager
from app.web.websocket.message_handler import MessageHandler
from app.web.streaming.agent_stream import StreamProcessor
from app.web.streaming.state_machine import AgentStateMachine
from app.web.routes import api_router


def _detect_context_usage(current_content: str, previous_messages: list) -> str:
    """æ£€æµ‹ AI æ˜¯å¦ä½¿ç”¨äº†ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¹¶ç”Ÿæˆä¸Šä¸‹æ–‡æç¤º"""
    if not previous_messages or len(previous_messages) < 3:
        return None

    # æ£€æµ‹å…³é”®è¯ï¼Œè¡¨ç¤ºä½¿ç”¨äº†ä¸Šä¸‹æ–‡ï¼ˆæ›´å…¨é¢çš„å…³é”®è¯åˆ—è¡¨ï¼‰
    context_keywords = [
        "æ ¹æ®", "åŸºäº", "ä¹‹å‰", "åˆšæ‰", "ä¹‹å‰æåˆ°", "ä¹‹å‰è¯´", "ä¹‹å‰åˆ†æ",
        "ä»ä¹‹å‰çš„", "æ ¹æ®ä¹‹å‰çš„", "åŸºäºä¹‹å‰çš„", "æ ¹æ®å¯¹è¯", "æ ¹æ®å†å²",
        "ä»å¯¹è¯ä¸­", "ä»å†å²", "ä¹‹å‰çš„å†…å®¹", "ä¹‹å‰çš„åˆ†æ", "ä¹‹å‰çš„å»ºè®®",
        "ä»æ‚¨", "æ‚¨ä¹‹å‰", "æ‚¨åˆšæ‰", "æ‚¨æåˆ°", "æ‚¨è¯´", "æ‚¨æåˆ°è¿‡",
        "ç°åœ¨", "æ¥ä¸‹æ¥", "ç»§ç»­", "æ¥ç€", "ç„¶å", "åŸºäºæ­¤",
        "ä»ç®€å†", "ç®€å†ä¸­", "å·¥ä½œç»å†", "æŠ€èƒ½", "é¡¹ç›®"
    ]

    content_lower = current_content.lower()
    has_context_keyword = any(keyword in content_lower for keyword in context_keywords)

    # å¦‚æœå†…å®¹å¾ˆçŸ­ï¼Œå¯èƒ½ä¸æ˜¯çœŸæ­£çš„ä¸Šä¸‹æ–‡ä½¿ç”¨
    if len(current_content.strip()) < 20:
        return None

    # æ£€æŸ¥æ˜¯å¦å¼•ç”¨äº†ä¹‹å‰çš„å·¥å…·è°ƒç”¨ç»“æœï¼ˆé€šè¿‡æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·ç›¸å…³çš„å…³é”®è¯ï¼‰
    tool_related_keywords = ["åˆ†æ", "ä¼˜åŒ–", "å»ºè®®", "é—®é¢˜", "äº®ç‚¹", "æ”¹è¿›", "ç®€å†", "å·¥ä½œç»å†"]
    has_tool_context = any(keyword in content_lower for keyword in tool_related_keywords)

    # å¦‚æœæ—¢æ²¡æœ‰ä¸Šä¸‹æ–‡å…³é”®è¯ï¼Œä¹Ÿæ²¡æœ‰å·¥å…·ç›¸å…³å…³é”®è¯ï¼Œå¯èƒ½ä¸æ˜¯ä¸Šä¸‹æ–‡ä½¿ç”¨
    if not has_context_keyword and not has_tool_context:
        return None

    # æå–ä¹‹å‰çš„å¯¹è¯å…³é”®ä¿¡æ¯
    context_summary = []

    # æŸ¥æ‰¾æœ€è¿‘çš„ç”¨æˆ·æ¶ˆæ¯ï¼ˆé—®é¢˜æˆ–è¯·æ±‚ï¼‰- åªæ‰¾éå·¥å…·è°ƒç”¨çš„ç”¨æˆ·æ¶ˆæ¯
    # æ’é™¤ç³»ç»Ÿæç¤ºè¯ç›¸å…³å†…å®¹
    system_keywords = ["å·¥å…·é€‰æ‹©", "æ ¹æ®ç”¨æˆ·è¾“å…¥", "## ", "**é‡è¦", "è§„åˆ™", "æ„å›¾è¯†åˆ«"]

    user_requests = []
    for msg in reversed(previous_messages[-20:]):  # æ£€æŸ¥æœ€è¿‘20æ¡æ¶ˆæ¯
        if msg.role == "user" and msg.content:
            user_content = msg.content.strip()
            # æ’é™¤ï¼šå¤ªçŸ­çš„æ¶ˆæ¯ã€å·¥å…·è°ƒç”¨ã€ç³»ç»Ÿæç¤ºè¯
            if len(user_content) > 5 and not user_content.startswith("{"):
                # æ£€æŸ¥æ˜¯å¦åƒç³»ç»Ÿæç¤ºè¯
                is_system_like = any(kw in user_content for kw in system_keywords)
                if not is_system_like and len(user_content) < 200:  # çœŸæ­£çš„ç”¨æˆ·è¯·æ±‚é€šå¸¸è¾ƒçŸ­
                    user_requests.append(user_content)
                    if len(user_requests) >= 2:  # æ”¶é›†æœ€è¿‘2æ¡ç”¨æˆ·æ¶ˆæ¯
                        break

    # æ·»åŠ æœ€è¿‘çš„ç”¨æˆ·è¯·æ±‚
    if user_requests:
        latest_request = user_requests[0]
        if len(latest_request) > 80:
            latest_request = latest_request[:80] + "..."
        context_summary.append(f"**æ‚¨çš„è¯·æ±‚**ï¼š{latest_request}")

    # æŸ¥æ‰¾æœ€è¿‘çš„ AI å›å¤ï¼ˆåˆ†ææˆ–ä¼˜åŒ–å»ºè®®ï¼‰- æŸ¥æ‰¾æœ‰å®é™…å†…å®¹çš„å›å¤
    ai_responses = []
    for msg in reversed(previous_messages[-20:]):
        if msg.role == "assistant" and msg.content and not msg.tool_calls:
            content = msg.content.strip()
            # æŸ¥æ‰¾åŒ…å«å…³é”®ä¿¡æ¯çš„å›å¤ï¼Œä¸”ä¸æ˜¯æ€è€ƒè¿‡ç¨‹
            if len(content) > 30 and any(keyword in content for keyword in ["åˆ†æ", "ä¼˜åŒ–", "å»ºè®®", "é—®é¢˜", "äº®ç‚¹", "æ”¹è¿›", "ç®€å†"]):
                # æå–å…³é”®ä¿¡æ¯ï¼ˆå–å‰100å­—ç¬¦ï¼‰
                key_info = content[:100].replace('\n', ' ').strip()
                # æ¸…ç† Markdown æ ¼å¼
                key_info = key_info.replace('**', '').replace('*', '').replace('#', '').strip()
                if len(key_info) > 20:
                    if len(key_info) > 100:
                        key_info = key_info[:100] + "..."
                    ai_responses.append(key_info)
                    if len(ai_responses) >= 1:  # åªå–æœ€è¿‘1æ¡æœ‰æ„ä¹‰çš„å›å¤
                        break

    # æ·»åŠ ä¹‹å‰çš„ AI åˆ†æ
    if ai_responses:
        context_summary.append(f"**ä¹‹å‰çš„åˆ†æ**ï¼š{ai_responses[0]}")

    # å¦‚æœæ‰¾åˆ°äº†ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œç”Ÿæˆæç¤º
    if context_summary:
        context_text = "\n".join(context_summary)
        return f"æ ¹æ®ä¹‹å‰çš„å¯¹è¯ï¼Œæˆ‘äº†è§£åˆ°ï¼š\n\n{context_text}"

    return None

# å®šä¹‰æ¶ˆæ¯ç±»å‹
class AgentMessage(BaseModel):
    type: str  # "thought", "tool_call", "tool_result", "answer", "error"
    content: Any
    step: int = 0


app = FastAPI(
    title="OpenManus API",
    description="Resume optimization agent with real-time streaming",
    version="2.0.0",
)

# å…è®¸è·¨åŸŸï¼ˆæ–¹ä¾¿å‰ç«¯å¼€å‘ï¼‰
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Include modular routes
app.include_router(api_router, prefix="/api")

# Create stream processor for agent execution
stream_processor = StreamProcessor()

# Create message handler
message_handler = MessageHandler(
    connection_manager=connection_manager,
    session_manager=session_manager,
    stream_processor=stream_processor,
)

# Legacy: Keep active_connections list for backward compatibility
active_connections = []

@app.get("/api/health")
async def health_check():
    return {"status": "ok"}


@app.post("/api/frontend-log")
async def log_frontend_event(event: dict):
    """æ¥æ”¶å‰ç«¯æ—¥å¿—å¹¶ä¿å­˜åˆ°æ–‡ä»¶"""
    try:
        from pathlib import Path
        from datetime import datetime

        # è·å–å½“å‰æ—¥æœŸ
        current_date = datetime.now().strftime("%Y%m%d")
        log_dir = Path(__file__).parent.parent.parent / "logs" / "frontend"
        log_dir.mkdir(parents=True, exist_ok=True)

        log_file = log_dir / f"{current_date}-frontend.log"

        # æ ¼å¼åŒ–æ—¥å¿—æ¡ç›®
        timestamp = event.get("timestamp", datetime.now().isoformat())
        level = event.get("level", "info").upper()
        message = event.get("message", "")
        data = event.get("data")

        # æ„å»ºæ—¥å¿—è¡Œ
        log_line = f"{timestamp} | {level} | {message}"
        if data:
            log_line += f" | {str(data)[:200]}"  # é™åˆ¶æ•°æ®é•¿åº¦

        # å†™å…¥æ—¥å¿—æ–‡ä»¶
        with open(log_file, "a", encoding="utf-8") as f:
            f.write(log_line + "\n")

        return {"status": "ok"}
    except Exception as e:
        logger.error(f"Failed to write frontend log: {e}")
        return {"status": "error", "message": str(e)}


# å…¨å±€ç®€å†æ•°æ®å­˜å‚¨ï¼ˆç”¨äºå‰åç«¯åŒæ­¥ï¼‰
_global_resume_data = {}


@app.get("/api/resume")
async def get_resume_data():
    """è·å–å½“å‰åŠ è½½çš„ç®€å†æ•°æ® - ä½¿ç”¨ parse_markdown_resume è§£æ"""
    from app.utils.resume_parser import parse_markdown_resume
    from pathlib import Path

    resume_path = Path("app/docs/éŸ¦å®‡_ç®€å†.md")

    if not resume_path.exists():
        return {"data": {}}

    try:
        data = parse_markdown_resume(str(resume_path))
        return {"data": data}
    except Exception as e:
        logger.error(f"Error parsing resume: {e}")
        return {"data": {}}


def _clean_resume_data(data: dict) -> dict:
    """æ¸…ç†ç®€å†æ•°æ®ï¼Œç¡®ä¿å¯ä»¥ JSON åºåˆ—åŒ–

    ç§»é™¤ Pydantic æ¨¡å‹çš„ç§æœ‰å±æ€§
    """
    if not isinstance(data, dict):
        return data

    result = {}
    for key, value in data.items():
        # è·³è¿‡ç§æœ‰å±æ€§å’Œç‰¹æ®Šå±æ€§
        if key.startswith("_") or key in ["__pydantic_private__", "__pydantic_extra__"]:
            continue
        if isinstance(value, dict):
            result[key] = _clean_resume_data(value)
        elif isinstance(value, list):
            result[key] = [_clean_resume_data(item) if isinstance(item, dict) else item for item in value]
        else:
            result[key] = value
    return result


@app.post("/api/resume")
async def set_resume_data(data: dict):
    """è®¾ç½®ç®€å†æ•°æ®"""
    global _global_resume_data
    _global_resume_data = data

    # åŒæ­¥æ›´æ–°åˆ°æ‰€æœ‰éœ€è¦ç®€å†æ•°æ®çš„å·¥å…·
    from app.tool.cv_reader_agent_tool import CVReaderAgentTool
    from app.tool.cv_analyzer_agent_tool import CVAnalyzerAgentTool
    from app.tool.cv_editor_agent_tool import CVEditorAgentTool

    CVReaderAgentTool.set_resume_data(_global_resume_data)
    CVAnalyzerAgentTool.set_resume_data(_global_resume_data)
    CVEditorAgentTool.set_resume_data(_global_resume_data)

    return {"success": True, "message": "Resume data updated"}


# å…¨å±€å­˜å‚¨ CheckpointSaver å’Œ ChatHistory å®ä¾‹
_global_checkpoint_saver = None
_global_chat_history = None


def get_checkpoint_saver():
    """è·å–å…¨å±€ CheckpointSaver å®ä¾‹"""
    global _global_checkpoint_saver
    if _global_checkpoint_saver is None:
        from app.memory import CheckpointSaver
        _global_checkpoint_saver = CheckpointSaver()
    return _global_checkpoint_saver


def get_chat_history_sync():
    """è·å–å…¨å±€ ChatHistory å®ä¾‹ (åŒæ­¥ç‰ˆæœ¬)"""
    global _global_chat_history
    if _global_chat_history is None:
        from app.memory import ChatHistoryManager
        _global_chat_history = ChatHistoryManager()
    return _global_chat_history


@app.get("/api/history/chat")
async def get_chat_history_api():
    """è·å–å¯¹è¯å†å²"""
    try:
        chat_history = get_chat_history_sync()
        messages = chat_history.get_messages()

        # è½¬æ¢ä¸ºå‰ç«¯æ ¼å¼
        history_messages = []
        for msg in messages:
            history_messages.append({
                "role": msg.role.value,  # user | assistant | system
                "content": msg.content or "",
                "timestamp": getattr(msg, 'timestamp', None)
            })

        return {"messages": history_messages}
    except Exception as e:
        logger.error(f"Error getting chat history: {e}")
        return {"messages": []}


@app.get("/api/history/checkpoints")
async def get_checkpoint_history():
    """è·å– Checkpoint ç‰ˆæœ¬å†å²"""
    try:
        checkpoint_saver = get_checkpoint_saver()
        versions = checkpoint_saver.list_versions()

        return {"checkpoints": versions}
    except Exception as e:
        logger.error(f"Error getting checkpoint history: {e}")
        return {"checkpoints": []}


@app.post("/api/history/rollback/{version}")
async def rollback_to_version(version: int):
    """å›æ»šåˆ°æŒ‡å®šç‰ˆæœ¬"""
    try:
        checkpoint_saver = get_checkpoint_saver()
        resume_snapshot = checkpoint_saver.rollback(version)

        if resume_snapshot:
            # æ›´æ–°å…¨å±€ç®€å†æ•°æ®
            global _global_resume_data
            _global_resume_data = {
                "raw_content": resume_snapshot.raw_content,
                "sections": resume_snapshot.sections
            }

            return {
                "success": True,
                "version": version,
                "message": f"å·²å›æ»šåˆ°ç‰ˆæœ¬ {version}"
            }
        else:
            return {"success": False, "message": "ç‰ˆæœ¬ä¸å­˜åœ¨"}
    except Exception as e:
        logger.error(f"Error rolling back to version {version}: {e}")
        return {"success": False, "message": str(e)}


@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    """WebSocket endpoint for agent interaction with streaming support.

    This endpoint handles real-time communication with the agent,
    streaming thoughts, tool calls, tool results, and answers.

    Architecture:
    - Uses ConnectionManager for connection lifecycle
    - Uses SessionManager for agent session management
    - Preserves ChatHistory with Tool messages for context
    """
    # Generate unique client ID for this connection
    client_id = str(uuid.uuid4())

    # Accept connection
    await connection_manager.connect(websocket, client_id)
    active_connections.append(websocket)

    # Get global ChatHistory
    global_chat_history = get_chat_history_sync()

    logger.info(f"WebSocket client connected: {client_id}")

    try:
        while True:
            # Receive message from client
            data = await websocket.receive_text()
            message = json.loads(data)

            message_type = message.get("type", "prompt")
            prompt = message.get("prompt", "")
            resume_path = message.get("cv_path") or message.get("resume_path")

            # Handle different message types
            if message_type == "prompt":
                if not prompt:
                    await connection_manager.send_to_client(
                        {"type": "error", "content": "Prompt is required"},
                        client_id
                    )
                    continue

                # Get or create session
                session = await session_manager.get_or_create_session(
                    client_id,
                    cv_path=resume_path,
                )

                # Restore ChatHistory messages to agent memory
                existing_messages = global_chat_history.get_messages()
                if existing_messages and len(session.agent.memory.messages) == 0:
                    logger.info(f"ğŸ“œ æ¢å¤ {len(existing_messages)} æ¡å†å²æ¶ˆæ¯åˆ° agent")
                    for msg in existing_messages:
                        # å¤„ç† roleï¼Œå¯èƒ½æ˜¯æšä¸¾æˆ–å­—ç¬¦ä¸²
                        role_value = msg.role.value if hasattr(msg.role, 'value') else str(msg.role)
                        if role_value == "user":
                            session.agent.memory.add_message(Message.user_message(msg.content))
                            logger.debug(f"  ğŸ“ æ¢å¤ USER æ¶ˆæ¯: {len(msg.content or '')} å­—ç¬¦")
                        elif role_value == "assistant":
                            # Assistant æ¶ˆæ¯å¯èƒ½åŒ…å« tool_calls
                            session.agent.memory.add_message(Message(
                                role=Role.ASSISTANT,
                                content=msg.content,
                                tool_calls=msg.tool_calls
                            ))
                            has_tools = bool(msg.tool_calls)
                            logger.debug(f"  ğŸ¤– æ¢å¤ ASSISTANT æ¶ˆæ¯: {len(msg.content or '')} å­—ç¬¦, tool_calls={has_tools}")
                        elif role_value == "tool":
                            # ğŸš¨ å…³é”®ä¿®å¤ï¼šæ¢å¤ Tool æ¶ˆæ¯ï¼ˆåŒ…å«ä¼˜åŒ–å»ºè®® JSONï¼‰
                            session.agent.memory.add_message(Message.tool_message(
                                content=msg.content,
                                name=msg.name or "unknown",
                                tool_call_id=msg.tool_call_id or ""
                            ))
                            logger.info(f"  ğŸ“‹ æ¢å¤ TOOL æ¶ˆæ¯: {msg.name}, {len(msg.content or '')} å­—ç¬¦")

                # Update resume path if provided
                if resume_path:
                    session.agent._current_resume_path = resume_path
                    logger.info(f"ğŸ“„ è®¾ç½®å½“å‰ç®€å†è·¯å¾„: {resume_path}")

                # Create state machine for this execution
                state_machine = AgentStateMachine(client_id)

                # Add user message to ChatHistory
                global_chat_history.add_message(Message(role=Role.USER, content=prompt))

                # Start streaming execution
                try:
                    session.is_running = True
                    session.reset_stop_event()

                    # Execute with streaming - send events directly
                    async for event in stream_processor.start_stream(
                        session_id=client_id,
                        agent=session.agent,
                        state_machine=state_machine,
                        event_sender=lambda d: None,  # Not used, events are yielded
                        user_message=prompt,
                        chat_history_manager=global_chat_history,
                    ):
                        # Convert event to dict and send
                        event_dict = event.to_dict()

                        # Add context detection for thought messages
                        if event_dict.get("type") == "thought":
                            content = event_dict.get("content", "")
                            context_info = _detect_context_usage(
                                content,
                                session.agent.memory.messages[:-1]
                            )
                            if context_info:
                                await connection_manager.send_to_client({
                                    "type": "context",
                                    "content": context_info
                                }, client_id)

                        await connection_manager.send_to_client(event_dict, client_id)

                except Exception as e:
                    logger.exception(f"[{client_id}] Error in stream processing: {e}")
                    await connection_manager.send_to_client(
                        {"type": "error", "content": str(e)},
                        client_id
                    )
                    session.is_running = False

            elif message_type == "restore_history":
                # Restore chat history from in-memory storage
                try:
                    messages = global_chat_history.get_messages()
                    await connection_manager.send_to_client({
                        "type": "history_restored",
                        "data": {
                            "message_count": len(messages),
                            "messages": [{"role": m.role, "content": m.content} for m in messages],
                        },
                    }, client_id)
                    logger.info(f"[{client_id}] History restored ({len(messages)} messages)")
                except Exception as e:
                    logger.exception(f"[{client_id}] Error restoring history: {e}")
                    await connection_manager.send_to_client(
                        {"type": "error", "content": f"Error restoring history: {e}"},
                        client_id
                    )

            elif message_type == "clear_history":
                # Clear chat history
                try:
                    global_chat_history.clear()
                    await connection_manager.send_to_client({
                        "type": "history_cleared",
                        "data": {"message": "Chat history cleared"},
                    }, client_id)
                    logger.info(f"[{client_id}] History cleared")
                except Exception as e:
                    logger.exception(f"[{client_id}] Error clearing history: {e}")
                    await connection_manager.send_to_client(
                        {"type": "error", "content": f"Error clearing history: {e}"},
                        client_id
                    )

            elif message_type == "stop":
                # Stop current execution
                await stream_processor.stop_stream(client_id)
                await session_manager.stop_session(client_id)
                await connection_manager.send_to_client({
                    "type": "stopped",
                    "data": {"message": "Agent execution stopped"},
                }, client_id)
                logger.info(f"[{client_id}] Agent stopped by user")

            else:
                await connection_manager.send_to_client(
                    {"type": "error", "content": f"Unknown message type: {message_type}"},
                    client_id
                )

    except WebSocketDisconnect:
        logger.info(f"WebSocket client disconnected: {client_id}")
    except Exception as e:
        logger.exception(f"Unexpected error in websocket endpoint for {client_id}: {e}")
    finally:
        # Cleanup
        connection_manager.disconnect(client_id)
        if websocket in active_connections:
            active_connections.remove(websocket)
        await session_manager.remove_session(client_id)

# è·å–é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
FRONTEND_DIST = os.path.join(PROJECT_ROOT, "frontend", "dist")

# æŒ‚è½½å‰ç«¯æ„å»ºäº§ç‰©
if os.path.exists(FRONTEND_DIST):
    app.mount("/", StaticFiles(directory=FRONTEND_DIST, html=True), name="static")
    logger.info(f"é™æ€æ–‡ä»¶å·²æŒ‚è½½: {FRONTEND_DIST}")
else:
    logger.warning(f"å‰ç«¯æ„å»ºç›®å½•ä¸å­˜åœ¨: {FRONTEND_DIST}")
    # æä¾›ä¸€ä¸ªç®€å•çš„é¦–é¡µ
    @app.get("/")
    async def root():
        return {"message": "OpenManus API æœåŠ¡å·²å¯åŠ¨", "note": "å‰ç«¯æœªæ„å»ºï¼Œè¯·å…ˆè¿è¡Œ cd frontend && npm run build"}

if __name__ == "__main__":
    import uvicorn
    PORT = 8000
    print("========================================")
    print("  OpenManus Web æœåŠ¡å™¨")
    print("========================================")
    print(f"å‰ç«¯ç›®å½•: {FRONTEND_DIST}")
    print(f"å‰ç«¯å­˜åœ¨: {os.path.exists(FRONTEND_DIST)}")
    print(f"è®¿é—®åœ°å€: http://localhost:{PORT}")
    print("========================================")
    uvicorn.run(app, host="0.0.0.0", port=PORT)

