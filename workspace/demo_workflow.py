#!/usr/bin/env python3
"""
ç»¼åˆæ¼”ç¤ºå·¥ä½œæµ
å±•ç¤ºOpenManuså·¥å…·ç»„åˆä½¿ç”¨çš„å·¥ä½œæµç¨‹
"""

import os
import sys
import json
from datetime import datetime
from pathlib import Path

def print_header(title):
    """æ‰“å°æ ‡é¢˜"""
    print("\n" + "=" * 60)
    print(f" {title}")
    print("=" * 60)

def step1_workspace_analysis():
    """æ­¥éª¤1ï¼šå·¥ä½œç©ºé—´åˆ†æ"""
    print_header("æ­¥éª¤1: å·¥ä½œç©ºé—´åˆ†æ")
    
    workspace_path = Path(__file__).parent
    
    print(f"ğŸ“ åˆ†æå·¥ä½œç©ºé—´: {workspace_path}")
    print()
    
    # è·å–æ–‡ä»¶åˆ—è¡¨
    files = []
    directories = []
    
    for item in workspace_path.iterdir():
        if item.is_file():
            size = item.stat().st_size
            files.append({
                'name': item.name,
                'size': size,
                'size_kb': size / 1024,
                'type': 'file'
            })
        else:
            directories.append({
                'name': item.name,
                'type': 'directory'
            })
    
    # æ˜¾ç¤ºç»“æœ
    print(f"å‘ç° {len(files)} ä¸ªæ–‡ä»¶, {len(directories)} ä¸ªç›®å½•")
    print()
    
    print("ğŸ“„ æ–‡ä»¶åˆ—è¡¨:")
    for file in sorted(files, key=lambda x: x['name'].lower()):
        print(f"  â€¢ {file['name']} ({file['size_kb']:.1f} KB)")
    
    print()
    
    # æŒ‰ç±»å‹åˆ†ç»„
    file_types = {}
    for file in files:
        ext = Path(file['name']).suffix.lower()
        file_types[ext] = file_types.get(ext, 0) + 1
    
    print("ğŸ“Š æ–‡ä»¶ç±»å‹åˆ†å¸ƒ:")
    for ext, count in sorted(file_types.items()):
        if ext:
            print(f"  {ext}: {count} ä¸ª")
    
    total_size = sum(f['size'] for f in files)
    print(f"\nğŸ’¾ æ€»å¤§å°: {total_size:,} bytes ({total_size/1024/1024:.2f} MB)")
    
    return {
        'files': files,
        'directories': directories,
        'file_types': file_types,
        'total_size': total_size
    }

def step2_resume_analysis():
    """æ­¥éª¤2ï¼šç®€å†åˆ†æ"""
    print_header("æ­¥éª¤2: ç®€å†åˆ†æ")
    
    resume_file = "æœ±å…†æ­¦_ç®€å†.md"
    resume_path = Path(__file__).parent / resume_file
    
    if not resume_path.exists():
        print(f"âŒ æ‰¾ä¸åˆ°ç®€å†æ–‡ä»¶: {resume_file}")
        return None
    
    print(f"ğŸ“„ åˆ†æç®€å†æ–‡ä»¶: {resume_file}")
    print()
    
    # ç®€å•åˆ†æç®€å†å†…å®¹
    try:
        with open(resume_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # åŸºæœ¬ç»Ÿè®¡
        lines = content.split('\n')
        words = content.split()
        
        # æå–å…³é”®éƒ¨åˆ†
        sections = []
        current_section = None
        
        for line in lines:
            if line.startswith('## '):
                if current_section:
                    sections.append(current_section)
                current_section = {
                    'title': line[3:].strip(),
                    'content': [],
                    'line_count': 0
                }
            elif current_section and line.strip():
                current_section['content'].append(line)
                current_section['line_count'] += 1
        
        if current_section:
            sections.append(current_section)
        
        # æ˜¾ç¤ºç»“æœ
        print(f"ğŸ“ ç®€å†ç»Ÿè®¡:")
        print(f"  â€¢ æ€»è¡Œæ•°: {len(lines)}")
        print(f"  â€¢ æ€»å­—æ•°: {len(words)}")
        print(f"  â€¢ æ€»å­—ç¬¦æ•°: {len(content)}")
        print(f"  â€¢ ç« èŠ‚æ•°: {len(sections)}")
        print()
        
        print("ğŸ“‘ ç« èŠ‚æ¦‚è§ˆ:")
        for section in sections:
            print(f"  â€¢ {section['title']} ({section['line_count']} è¡Œ)")
        
        # æå–æŠ€æœ¯å…³é”®è¯
        tech_keywords = [
            'Java', 'Python', 'Spring', 'MySQL', 'Redis', 'Docker', 
            'Kubernetes', 'AWS', 'å¾®æœåŠ¡', 'æ¶æ„', 'ä¼˜åŒ–'
        ]
        
        found_tech = []
        for keyword in tech_keywords:
            if keyword in content:
                found_tech.append(keyword)
        
        print(f"\nğŸ’» å‘ç°çš„æŠ€æœ¯å…³é”®è¯: {len(found_tech)} ä¸ª")
        if found_tech:
            print("  " + ", ".join(found_tech))
        
        return {
            'file': resume_file,
            'lines': len(lines),
            'words': len(words),
            'chars': len(content),
            'sections': len(sections),
            'tech_keywords': found_tech,
            'section_titles': [s['title'] for s in sections]
        }
        
    except Exception as e:
        print(f"âŒ åˆ†æç®€å†æ—¶å‡ºé”™: {e}")
        return None

def step3_script_analysis():
    """æ­¥éª¤3ï¼šè„šæœ¬åˆ†æ"""
    print_header("æ­¥éª¤3: Pythonè„šæœ¬åˆ†æ")
    
    workspace_path = Path(__file__).parent
    python_files = list(workspace_path.glob("*.py"))
    
    print(f"ğŸ” å‘ç° {len(python_files)} ä¸ªPythonè„šæœ¬:")
    
    script_analysis = []
    
    for py_file in sorted(python_files, key=lambda x: x.name.lower()):
        try:
            with open(py_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # åŸºæœ¬ç»Ÿè®¡
            lines = content.split('\n')
            code_lines = [l for l in lines if l.strip() and not l.strip().startswith('#')]
            comment_lines = [l for l in lines if l.strip().startswith('#')]
            
            # æå–å‡½æ•°å®šä¹‰
            functions = []
            for line in lines:
                if line.strip().startswith('def '):
                    func_name = line.strip()[4:].split('(')[0]
                    functions.append(func_name)
            
            # æå–å¯¼å…¥
            imports = []
            for line in lines:
                if line.strip().startswith('import ') or line.strip().startswith('from '):
                    imports.append(line.strip())
            
            analysis = {
                'file': py_file.name,
                'total_lines': len(lines),
                'code_lines': len(code_lines),
                'comment_lines': len(comment_lines),
                'comment_ratio': len(comment_lines) / len(lines) * 100 if lines else 0,
                'functions': len(functions),
                'imports': len(imports),
                'size': py_file.stat().st_size
            }
            
            script_analysis.append(analysis)
            
            print(f"\nğŸ“œ {py_file.name}:")
            print(f"  â€¢ æ€»è¡Œæ•°: {analysis['total_lines']}")
            print(f"  â€¢ ä»£ç è¡Œ: {analysis['code_lines']}")
            print(f"  â€¢ æ³¨é‡Šè¡Œ: {analysis['comment_lines']} ({analysis['comment_ratio']:.1f}%)")
            print(f"  â€¢ å‡½æ•°æ•°: {analysis['functions']}")
            print(f"  â€¢ å¯¼å…¥æ•°: {analysis['imports']}")
            print(f"  â€¢ æ–‡ä»¶å¤§å°: {analysis['size']:,} bytes")
            
            if functions:
                print(f"  â€¢ å‡½æ•°åˆ—è¡¨: {', '.join(functions[:5])}" + ("..." if len(functions) > 5 else ""))
            
        except Exception as e:
            print(f"âŒ åˆ†æ {py_file.name} æ—¶å‡ºé”™: {e}")
    
    return script_analysis

def step4_generate_report(workspace_data, resume_data, scripts_data):
    """æ­¥éª¤4ï¼šç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
    print_header("æ­¥éª¤4: ç”Ÿæˆç»¼åˆæŠ¥å‘Š")
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_file = f"comprehensive_analysis_{timestamp}.json"
    
    report = {
        'generated_at': datetime.now().isoformat(),
        'workflow': 'OpenManusç»¼åˆæ¼”ç¤ºå·¥ä½œæµ',
        'workspace_analysis': workspace_data,
        'resume_analysis': resume_data,
        'script_analysis': scripts_data,
        'summary': {
            'total_files': len(workspace_data.get('files', [])),
            'total_python_scripts': len(scripts_data),
            'resume_sections': resume_data.get('sections', 0) if resume_data else 0,
            'total_script_lines': sum(s.get('total_lines', 0) for s in scripts_data),
            'total_script_functions': sum(s.get('functions', 0) for s in scripts_data)
        },
        'insights': []
    }
    
    # ç”Ÿæˆæ´å¯Ÿ
    insights = []
    
    # å·¥ä½œç©ºé—´æ´å¯Ÿ
    if workspace_data:
        total_size_mb = workspace_data.get('total_size', 0) / 1024 / 1024
        insights.append(f"å·¥ä½œç©ºé—´åŒ…å« {len(workspace_data.get('files', []))} ä¸ªæ–‡ä»¶ï¼Œæ€»å¤§å° {total_size_mb:.2f} MB")
    
    # ç®€å†æ´å¯Ÿ
    if resume_data:
        insights.append(f"ç®€å†åŒ…å« {resume_data.get('sections', 0)} ä¸ªç« èŠ‚ï¼Œ{resume_data.get('words', 0)} ä¸ªå­—")
        insights.append(f"ç®€å†ä¸­æåˆ° {len(resume_data.get('tech_keywords', []))} ä¸ªå…³é”®æŠ€æœ¯")
    
    # è„šæœ¬æ´å¯Ÿ
    if scripts_data:
        total_functions = sum(s.get('functions', 0) for s in scripts_data)
        avg_comment_ratio = sum(s.get('comment_ratio', 0) for s in scripts_data) / len(scripts_data)
        insights.append(f"å…±æœ‰ {len(scripts_data)} ä¸ªPythonè„šæœ¬ï¼ŒåŒ…å« {total_functions} ä¸ªå‡½æ•°")
        insights.append(f"å¹³å‡æ³¨é‡Šæ¯”ä¾‹: {avg_comment_ratio:.1f}%")
    
    report['insights'] = insights
    
    # ä¿å­˜æŠ¥å‘Š
    try:
        report_path = Path(__file__).parent / report_file
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… ç»¼åˆæŠ¥å‘Šå·²ç”Ÿæˆ:")
        print(f"   æ–‡ä»¶: {report_file}")
        print(f"   å¤§å°: {report_path.stat().st_size:,} bytes")
        print()
        
        print("ğŸ“‹ æŠ¥å‘Šæ‘˜è¦:")
        print(f"  â€¢ ç”Ÿæˆæ—¶é—´: {report['generated_at']}")
        print(f"  â€¢ å·¥ä½œæµ: {report['workflow']}")
        print()
        
        print("ğŸ” å…³é”®æ´å¯Ÿ:")
        for i, insight in enumerate(insights, 1):
            print(f"  {i}. {insight}")
        
        return report_file
        
    except Exception as e:
        print(f"âŒ ç”ŸæˆæŠ¥å‘Šæ—¶å‡ºé”™: {e}")
        return None

def step5_create_readme():
    """æ­¥éª¤5ï¼šåˆ›å»ºREADMEæ–‡æ¡£"""
    print_header("æ­¥éª¤5: åˆ›å»ºREADMEæ–‡æ¡£")
    
    readme_content = """# OpenManus å·¥ä½œç©ºé—´åˆ†ææŠ¥å‘Š

## æ¦‚è¿°
æœ¬æŠ¥å‘Šç”±OpenManusç»¼åˆæ¼”ç¤ºå·¥ä½œæµè‡ªåŠ¨ç”Ÿæˆï¼Œå±•ç¤ºäº†å·¥ä½œç©ºé—´åˆ†æã€ç®€å†åˆ†æå’Œè„šæœ¬åˆ†æçš„èƒ½åŠ›ã€‚

## ç”Ÿæˆçš„æ–‡ä»¶

### 1. åˆ†æè„šæœ¬
- `analyze_workspace.py` - å·¥ä½œç©ºé—´åˆ†æå·¥å…·
- `resume_analyzer.py` - ç®€å†åˆ†æå·¥å…·  
- `demo_workflow.py` - ç»¼åˆæ¼”ç¤ºå·¥ä½œæµè„šæœ¬

### 2. æ¼”ç¤ºè„šæœ¬
- `demo_script.py` - åŸºç¡€æ¼”ç¤ºè„šæœ¬
- `enhanced_script.py` - å¢å¼ºç‰ˆæ¼”ç¤ºè„šæœ¬

### 3. æ•°æ®æ–‡ä»¶
- `æœ±å…†æ­¦_ç®€å†.md` - ç¤ºä¾‹ç®€å†æ–‡ä»¶
- `example.txt` - ç¤ºä¾‹æ–‡æœ¬æ–‡ä»¶

## å·¥å…·èƒ½åŠ›å±•ç¤º

### æ–‡ä»¶å¤„ç†èƒ½åŠ›
- æ–‡ä»¶æŸ¥çœ‹å’Œç¼–è¾‘
- æ–‡ä»¶å†…å®¹åˆ†æ
- æ–‡ä»¶ç³»ç»Ÿæ‰«æ
- æŠ¥å‘Šç”Ÿæˆ

### æ•°æ®åˆ†æèƒ½åŠ›
- æ–‡æœ¬å†…å®¹åˆ†æ
- ç»“æ„åˆ†æ
- ç»Ÿè®¡è®¡ç®—
- å¯è§†åŒ–è¾“å‡º

### è‡ªåŠ¨åŒ–èƒ½åŠ›
- æ‰¹é‡æ–‡ä»¶å¤„ç†
- æ•°æ®æå–å’Œè½¬æ¢
- æŠ¥å‘Šè‡ªåŠ¨ç”Ÿæˆ
- å·¥ä½œæµè‡ªåŠ¨åŒ–

## ä½¿ç”¨è¯´æ˜

### è¿è¡Œå·¥ä½œç©ºé—´åˆ†æ
```bash
python analyze_workspace.py
```

### è¿è¡Œç®€å†åˆ†æ
```bash
python resume_analyzer.py
```

### è¿è¡Œç»¼åˆæ¼”ç¤º
```bash
python demo_workflow.py
```

## æŠ€æœ¯æ ˆ
- Python 3.x
- æ ‡å‡†åº“: os, json, re, datetime, pathlib
- æ­£åˆ™è¡¨è¾¾å¼å¤„ç†
- æ–‡ä»¶ç³»ç»Ÿæ“ä½œ

## ç”Ÿæˆæ—¶é—´
{timestamp}

---

*æœ¬æŠ¥å‘Šç”±OpenManus AIåŠ©æ‰‹è‡ªåŠ¨ç”Ÿæˆ*
"""
    
    # æ›¿æ¢æ—¶é—´æˆ³
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    readme_content = readme_content.replace("{timestamp}", timestamp)
    
    # ä¿å­˜READMEæ–‡ä»¶
    readme_file = "README_ANALYSIS.md"
    readme_path = Path(__file__).parent / readme_file
    
    try:
        with open(readme_path, 'w', encoding='utf-8') as f:
            f.write(readme_content)
        
        print(f"âœ… READMEæ–‡æ¡£å·²åˆ›å»º:")
        print(f"   æ–‡ä»¶: {readme_file}")
        print(f"   å¤§å°: {readme_path.stat().st_size:,} bytes")
        print()
        
        print("ğŸ“– æ–‡æ¡£å†…å®¹é¢„è§ˆ:")
        print("-" * 40)
        lines = readme_content.split('\n')[:15]
        for line in lines:
            print(line)
        print("...")
        print("-" * 40)
        
        return readme_file
        
    except Exception as e:
        print(f"âŒ åˆ›å»ºREADMEæ—¶å‡ºé”™: {e}")
        return None

def main():
    """ä¸»å·¥ä½œæµ"""
    print("ğŸš€ OpenManus ç»¼åˆæ¼”ç¤ºå·¥ä½œæµ")
    print("=" * 60)
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    # æ‰§è¡Œæ‰€æœ‰æ­¥éª¤
    print("ğŸ“Š å¼€å§‹æ‰§è¡Œåˆ†æå·¥ä½œæµ...")
    print()
    
    # æ­¥éª¤1: å·¥ä½œç©ºé—´åˆ†æ
    workspace_data = step1_workspace_analysis()
    
    # æ­¥éª¤2: ç®€å†åˆ†æ
    resume_data = step2_resume_analysis()
    
    # æ­¥éª¤3: è„šæœ¬åˆ†æ
    scripts_data = step3_script_analysis()
    
    # æ­¥éª¤4: ç”Ÿæˆç»¼åˆæŠ¥å‘Š
    if workspace_data or resume_data or scripts_data:
        report_file = step4_generate_report(workspace_data, resume_data, scripts_data)
    else:
        print("âš ï¸ æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®ç”ŸæˆæŠ¥å‘Š")
        report_file = None
    
    # æ­¥éª¤5: åˆ›å»ºREADME
    readme_file = step5_create_readme()
    
    # å®Œæˆæ€»ç»“
    print_header("å·¥ä½œæµå®Œæˆ")
    
    print("ğŸ‰ æ‰€æœ‰æ­¥éª¤å·²å®Œæˆ!")
    print()
    
    print("ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    if report_file:
        print(f"  â€¢ ç»¼åˆæŠ¥å‘Š: {report_file}")
    if readme_file:
        print(f"  â€¢ æ–‡æ¡£æ–‡ä»¶: {readme_file}")
    
    print()
    print("ğŸ› ï¸ ä½¿ç”¨çš„å·¥å…·:")
    print("  â€¢ æ–‡ä»¶æŸ¥çœ‹å’Œç¼–è¾‘å·¥å…·")
    print("  â€¢ Pythonæ‰§è¡Œç¯å¢ƒ")
    print("  â€¢ æ•°æ®åˆ†æç®—æ³•")
    print("  â€¢ æŠ¥å‘Šç”Ÿæˆç³»ç»Ÿ")
    
    print()
    print(f"â±ï¸ å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)

if __name__ == "__main__":
    main()